{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04FLQcjuIdFw"
   },
   "source": [
    "#AI in Full Bloom: Classifying Iris Flowers with Code\n",
    "\n",
    "After the workshop, you must be wondering how **you** could be making your own AIs and using them to solve problems you care about. Fear not! As I, Isita, have prepared this *lovely*, if I do say so myself,  Google Collab Notebook, with a tutorial for an Artificial Neural Network and Decision Tree Classifier. This code will be used on a public dataset of Irises, as it is a simple dataset to use for beginners. With your own projects, you can find datasets about other problems, like pollution or energy usage. \n",
    "\n",
    "#Let us now Embark on Our Journey!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSmvLY6KkYwW"
   },
   "source": [
    "## The Iris Dataset\n",
    "\n",
    "Before we get started with using scikit-learn, we need to decide what machine learning task we want to accomplish with it. \n",
    "\n",
    "In this workshop, we'll use scikit-learn for Classification using the **Artificial Neural Network** algorithm. We will also be trying the **Decision Tree** algorithm in this notebook. We'll use a popular sample dataset often referred to as \"the Iris dataset\" that was made specifically for machine learning algorithms([Link to dataset here](https://archive.ics.uci.edu/ml/datasets/Iris))\n",
    "\n",
    "There are different species of ([Iris](https://en.wikipedia.org/wiki/Iris_(plant))) flowers. Our goal is to train a machine learning system to be able to take a new Iris flower and predict which Iris species it is.\n",
    "\n",
    "![alt text](https://cdn.pixabay.com/photo/2015/05/26/13/57/flower-784688__340.jpg)\n",
    "\n",
    "This dataset has:\n",
    "\n",
    "* 150 examples\n",
    "* 3 classes: setosa, versicolor, and virginica\n",
    "* 4 features: sepal length, sepal width, petal length, petal width\n",
    "* 50 examples for each class\n",
    "\n",
    "This dataset can be imported directly from Sci-kit Learn, the python module. That way we do not have to upload a huge file.\n",
    "\n",
    "Let's load this dataset using scikit-learn, which is called `sklearn` in Python.\n",
    "\n",
    "#Importing and Cleaning Up the Dataset\n",
    "\n",
    "Here we are importing all the libraries we will need to access functions from and loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "of55AT7RLKtW",
    "outputId": "93316558-3c4a-4ba0-c4b1-01b4b8ddddf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "#Import required libraries \n",
    "import keras #library for neural network\n",
    "import pandas as pd #loading data in table form  \n",
    "import seaborn as sns #visualisation \n",
    "import matplotlib.pyplot as plt #visualisation\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.preprocessing import normalize #machine learning algorithm library\n",
    "\n",
    "#  Load the dataset, which contains the data points(sepal length, petal length, etc) and corresponding labels(type of iris)\n",
    "iris_dataset=pd.read_csv(\"https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv\")\n",
    "print(\"success\")\n",
    "\n",
    "# #This is a debug statement to make sure we uploaded the dataset correctly. \n",
    "# #We can comment it out when we actually run the code.\n",
    "# #print(iris_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width species\n",
       "0             5.1          3.5           1.4          0.2       0\n",
       "1             4.9          3.0           1.4          0.2       0\n",
       "2             4.7          3.2           1.3          0.2       0\n",
       "3             4.6          3.1           1.5          0.2       0\n",
       "4             5.0          3.6           1.4          0.2       0\n",
       "..            ...          ...           ...          ...     ...\n",
       "145           6.7          3.0           5.2          2.3       2\n",
       "146           6.3          2.5           5.0          1.9       2\n",
       "147           6.5          3.0           5.2          2.0       2\n",
       "148           6.2          3.4           5.4          2.3       2\n",
       "149           5.9          3.0           5.1          1.8       2\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_dataset.loc[iris_dataset[\"species\"]==\"setosa\",\"species\"]=0\n",
    "iris_dataset.loc[iris_dataset[\"species\"]==\"versicolor\",\"species\"]=1\n",
    "iris_dataset.loc[iris_dataset[\"species\"]==\"virginica\",\"species\"]=2\n",
    "iris_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YJx9mLLVh_0"
   },
   "source": [
    "## Visualizing the Data\n",
    "\n",
    "To get a sense of how our data is distributed, we can plot our data based on the different features, such as Sepal Length, Petal Width, etc. Based on the graphs, we can also see what the general characterisitics of each type of iris are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5-F950V7VaTx",
    "outputId": "240e66ae-c232-4779-8ecf-db2df5d7e290"
   },
   "outputs": [],
   "source": [
    "sns.lmplot('sepal_length', 'sepal_width',\n",
    "           data=iris_dataset,\n",
    "           fit_reg=False,\n",
    "           hue=\"species\",\n",
    "           scatter_kws={\"marker\": \"D\",\n",
    "                        \"s\": 50})\n",
    "plt.title('SepalLength vs SepalWidth')\n",
    "\n",
    "sns.lmplot('petal_length', 'petal_width',\n",
    "           data=iris_dataset,\n",
    "           fit_reg=False,\n",
    "           hue=\"species\",\n",
    "           scatter_kws={\"marker\": \"D\",\n",
    "                        \"s\": 50})\n",
    "plt.title('PetalLength vs PetalWidth')\n",
    "\n",
    "sns.lmplot('sepal_length', 'petal_length',\n",
    "           data=iris_dataset,\n",
    "           fit_reg=False,\n",
    "           hue=\"species\",\n",
    "           scatter_kws={\"marker\": \"D\",\n",
    "                        \"s\": 50})\n",
    "plt.title('SepalLength vs PetalLength')\n",
    "\n",
    "sns.lmplot('sepal_width', 'petal_width',\n",
    "           data=iris_dataset,\n",
    "           fit_reg=False,\n",
    "           hue=\"species\",\n",
    "           scatter_kws={\"marker\": \"D\",\n",
    "                        \"s\": 50})\n",
    "plt.title('SepalWidth vs PetalWidth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A27Q-iC8Ldwl"
   },
   "source": [
    "Notice that each species cluster is in a somewhat distinct section of the graph. From this we find general parameters to guess the type of iris ourself. Example: If my flower has a small Sepal Length and a Small Petal Length what type might it be?(Look at the graphs)\n",
    "\n",
    "Answer: It's proabaly species 0 or 'Setosa' because in the second graph, Sepal Length' versus 'Petal Length', the blue dots are clustered near the origin.\n",
    "\n",
    "##ANN: Splitting up the training and test sets\n",
    "\n",
    "Remember that in ANN classification,  a type of supervised machine learning, we must use a training set to teach our model how to correctly classify future examples. We also use a test set to test how good our model is.\n",
    "\n",
    "The first step that we'll do is break up the Iris dataset into training set and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LqYJ3_PvM2fF"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# Break the dataset up into the examples (X) and their labels (y)\n",
    "X = iris_dataset.iloc[:, 0:4].values\n",
    "y = iris_dataset.iloc[:, 4].values\n",
    "scaler = preprocessing.Normalizer().fit(X)\n",
    "X = scaler.transform(X)\n",
    "# X=normalize(X,axis=0)\n",
    "\n",
    "# Split up the X and y datasets randomly into train and test sets\n",
    "# 20% of the dataset will be used for the test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=31)\n",
    "\n",
    "#Change the label to one hot vector\n",
    "'''\n",
    "[0]--->[1 0 0]\n",
    "[1]--->[0 1 0]\n",
    "[2]--->[0 0 1]\n",
    "'''\n",
    "y_train=np_utils.to_categorical(y_train,num_classes=3)\n",
    "y_test=np_utils.to_categorical(y_test,num_classes=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdqolJoRNV--"
   },
   "source": [
    "Our training set contains all of the correct targets (classes) for our flower examples, along with the four features of each flower. We'll need all of this information to teach our classifier how to predict a class given a new set of four features.\n",
    "\n",
    "**Note: Because the data points are split randomly into train and test, each run might not be the same**\n",
    "\n",
    "## ANN: Building our Network\n",
    "\n",
    "Now that we've split our data into a training and test set, it's time to build our ANN.\n",
    "\n",
    "As you may already know Neural Networks look like this:\n",
    "\n",
    "\n",
    "![alt text](https://www.researchgate.net/profile/Facundo_Bre/publication/321259051/figure/fig1/AS:614329250496529@1523478915726/Artificial-neural-network-architecture-ANN-i-h-1-h-2-h-n-o.png)\n",
    "\n",
    "\n",
    " Here we create the Neural Newtork Framework, with how many layers, activation functions, etc. You can experiment with the numbers as much as you want, but you might want to research \"activation functions\" before changing those. \n",
    "\n",
    " We choose most of the parameters, except for the weights associated with each node. Those weights are learned by the netwrok thorugh training later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "UIhZ76JtN-Sv",
    "outputId": "1020dbc7-0327-4dbe-c235-df409bb4b211"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 21:03:46.246197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-13 21:03:46.249779: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-13 21:03:46.249886: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-09-13 21:03:46.249929: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-09-13 21:03:46.249982: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-09-13 21:03:46.250043: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-09-13 21:03:46.250124: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-09-13 21:03:46.250190: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-09-13 21:03:46.250226: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-09-13 21:03:46.250272: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-09-13 21:03:46.252785: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense,Activation,Dropout \n",
    "\n",
    "\n",
    "# Initialising the ANN\n",
    "model = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "model.add(Dense(1000,input_dim=4,activation='relu'))\n",
    "model.add(Dense(50,activation='relu'))\n",
    "\n",
    "#Protects against overfitting\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Adding the output layer\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "\n",
    "# Compiling the ANN\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48fO2NVQCk4u"
   },
   "source": [
    "\n",
    "## ANN: Training our Network\n",
    "\n",
    "We have built the Neural Network and now we need to train it with our training data. The training process adjusts the weights of the nodes to match the relations between the features and labels of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "id": "K50b3ybqClhZ",
    "outputId": "39e16ee9-d950-4c18-c8f9-432848592042"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 1.0786 - accuracy: 0.3250 - val_loss: 1.0397 - val_accuracy: 0.3333\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.0372 - accuracy: 0.5167 - val_loss: 1.0043 - val_accuracy: 0.6000\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.9863 - accuracy: 0.6583 - val_loss: 0.9500 - val_accuracy: 0.6000\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.9248 - accuracy: 0.6750 - val_loss: 0.8987 - val_accuracy: 0.6000\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.8727 - accuracy: 0.6750 - val_loss: 0.8458 - val_accuracy: 0.6000\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.8010 - accuracy: 0.7333 - val_loss: 0.7853 - val_accuracy: 0.6000\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.7530 - accuracy: 0.7000 - val_loss: 0.7306 - val_accuracy: 0.6000\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.6804 - accuracy: 0.6917 - val_loss: 0.6733 - val_accuracy: 0.6000\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.6302 - accuracy: 0.6667 - val_loss: 0.6274 - val_accuracy: 0.6000\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.5877 - accuracy: 0.6667 - val_loss: 0.5875 - val_accuracy: 0.6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f02087bbbb0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "model.fit(X_train,y_train,validation_data=(X_test,y_test),batch_size=20,epochs=10,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AavnLV4HOGTL"
   },
   "source": [
    "##ANN: Prediction Accuracy\n",
    "Now let's predict some values using the ANN we just created and trained. We will use the Neural Network Structure, which has been trained and assigned weights on the nodes based on the patterns in the training data, to predict the results of the datapoints in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "aoztGEDkOJSA",
    "outputId": "50ac676f-28b8-45a4-c13d-9a42873d3166"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Accuracy of the dataset 60.0\n"
     ]
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "prediction=model.predict(X_test)\n",
    "length=len(prediction)\n",
    "y_label=np.argmax(y_test,axis=1)\n",
    "predict_label=np.argmax(prediction,axis=1)\n",
    "#how times it matched/ how many test cases\n",
    "accuracy=np.sum(y_label==predict_label)/length * 100 \n",
    "print(\"Accuracy of the dataset\",accuracy )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "olT2jSPpAYv4"
   },
   "source": [
    "##ANN: Tuning the Parameters\n",
    "\n",
    "Artificial Neural Networks have many different places to experiment. The Neural Network Structure can be altered and has a significant impact on the performance. Hidden Layers can be added or delete and many different values can be put in. You can also change the training parameters  such as, epoch number, batch size, and other inputs to see what brings out the best accuracy.  \n",
    "\n",
    "Tuning both structural and training parameters is an essential step in developing a neural network based soultion for any problem.\n",
    "\n",
    "While this variability of parameters makes it so that ANN's can be tuned to high degrees of perfection, finding the same patterns that ANNs discern and understanding it's choices is hard for humans. **Sometimes** AI is called a [\"Black Box Technique\"](https://towardsdatascience.com/machine-learning-how-black-is-this-black-box-f11e4031fdf)\n",
    "\n",
    "---\n",
    "Let's try adding a second hidden layer and see if it improves our accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moddify existing model to match triton server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array([[[i]]for i in X_test])\n",
    "X_train = np.array([[[i]]for i in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "Jf5tH5stAWbg",
    "outputId": "538de22f-c9be-4e28-d882-6fc25dab1ba8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 1.0860 - accuracy: 0.5500 - val_loss: 1.0651 - val_accuracy: 0.6000\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.0311 - accuracy: 0.6917 - val_loss: 0.9938 - val_accuracy: 0.6000\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.9261 - accuracy: 0.6833 - val_loss: 0.8789 - val_accuracy: 0.6000\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.7738 - accuracy: 0.7250 - val_loss: 0.7276 - val_accuracy: 0.6000\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.6159 - accuracy: 0.7000 - val_loss: 0.5937 - val_accuracy: 0.6000\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.4773 - accuracy: 0.7583 - val_loss: 0.5002 - val_accuracy: 0.7667\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.4007 - accuracy: 0.9000 - val_loss: 0.4336 - val_accuracy: 0.9000\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.3507 - accuracy: 0.9333 - val_loss: 0.3845 - val_accuracy: 0.9000\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.3124 - accuracy: 0.9000 - val_loss: 0.3650 - val_accuracy: 0.8667\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.2657 - accuracy: 0.9583 - val_loss: 0.3033 - val_accuracy: 0.9333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f01e40de520>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense,Activation,Dropout,Input,Flatten\n",
    "\n",
    "\n",
    "# Initialising the ANN\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(1,1,4)))\n",
    "model.add(Flatten())\n",
    "# Adding the input layer and the first hidden layer\n",
    "model.add(Dense(1000,activation='relu'))\n",
    "#Changing number of nodes in first hidden layer\n",
    "model.add(Dense(50,activation='relu'))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "model.add(Dense(300,activation='relu'))\n",
    "#Protects against overfitting\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Adding the output layer\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "\n",
    "# Compiling the ANN\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "model.fit(X_train,y_train,validation_data=(X_test,y_test),batch_size=20,epochs=10,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 4)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1000)              5000      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 50)                50050     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 300)               15300     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 300)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 3)                 903       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 71,253\n",
      "Trainable params: 71,253\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Ai99vw6MA0Dz",
    "outputId": "d1aae79e-d360-4d6a-80c7-7489dd57ebe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Accuracy of the dataset 93.33333333333333\n"
     ]
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "prediction=model.predict(X_test)\n",
    "length=len(prediction)\n",
    "y_label=np.argmax(y_test,axis=1)\n",
    "predict_label=np.argmax(prediction,axis=1)\n",
    "#how times it matched/ how many test cases\n",
    "accuracy=np.sum(y_label==predict_label)/length * 100 \n",
    "print(\"Accuracy of the dataset\",accuracy )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, 1, 2, 0, 2, 1, 0, 0, 2, 1, 2, 0, 2, 1, 1, 1, 2, 0, 2, 2,\n",
       "       0, 2, 1, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.70631892, 0.37838513, 0.5675777 , 0.18919257]]],\n",
       "\n",
       "\n",
       "       [[[0.69417747, 0.30370264, 0.60740528, 0.2386235 ]]],\n",
       "\n",
       "\n",
       "       [[[0.79594782, 0.55370283, 0.24224499, 0.03460643]]],\n",
       "\n",
       "\n",
       "       [[[0.73446047, 0.37367287, 0.5411814 , 0.16750853]]],\n",
       "\n",
       "\n",
       "       [[[0.71718148, 0.31640359, 0.58007326, 0.22148252]]],\n",
       "\n",
       "\n",
       "       [[[0.82210585, 0.51381615, 0.23978087, 0.05138162]]],\n",
       "\n",
       "\n",
       "       [[[0.73122464, 0.31338199, 0.56873028, 0.20892133]]],\n",
       "\n",
       "\n",
       "       [[[0.73239618, 0.38547167, 0.53966034, 0.15418867]]],\n",
       "\n",
       "\n",
       "       [[[0.77381111, 0.59732787, 0.2036345 , 0.05430253]]],\n",
       "\n",
       "\n",
       "       [[[0.78591858, 0.57017622, 0.23115252, 0.06164067]]],\n",
       "\n",
       "\n",
       "       [[[0.69385414, 0.29574111, 0.63698085, 0.15924521]]],\n",
       "\n",
       "\n",
       "       [[[0.71524936, 0.40530797, 0.53643702, 0.19073316]]],\n",
       "\n",
       "\n",
       "       [[[0.71066905, 0.35533453, 0.56853524, 0.21320072]]],\n",
       "\n",
       "\n",
       "       [[[0.8025126 , 0.55989251, 0.20529392, 0.01866308]]],\n",
       "\n",
       "\n",
       "       [[[0.69299099, 0.34199555, 0.60299216, 0.19799743]]],\n",
       "\n",
       "\n",
       "       [[[0.70779525, 0.31850786, 0.60162596, 0.1887454 ]]],\n",
       "\n",
       "\n",
       "       [[[0.74143307, 0.29421947, 0.57667016, 0.17653168]]],\n",
       "\n",
       "\n",
       "       [[[0.73350949, 0.35452959, 0.55013212, 0.18337737]]],\n",
       "\n",
       "\n",
       "       [[[0.67767924, 0.32715549, 0.59589036, 0.28041899]]],\n",
       "\n",
       "\n",
       "       [[[0.80642366, 0.5315065 , 0.25658935, 0.03665562]]],\n",
       "\n",
       "\n",
       "       [[[0.69589887, 0.34794944, 0.57629125, 0.25008866]]],\n",
       "\n",
       "\n",
       "       [[[0.65387747, 0.34250725, 0.62274045, 0.25947519]]],\n",
       "\n",
       "\n",
       "       [[[0.78889479, 0.55222635, 0.25244633, 0.09466737]]],\n",
       "\n",
       "\n",
       "       [[[0.69594002, 0.30447376, 0.60894751, 0.22835532]]],\n",
       "\n",
       "\n",
       "       [[[0.76521855, 0.33391355, 0.52869645, 0.15304371]]],\n",
       "\n",
       "\n",
       "       [[[0.80533308, 0.54831188, 0.2227517 , 0.03426949]]],\n",
       "\n",
       "\n",
       "       [[[0.75457341, 0.34913098, 0.52932761, 0.16893434]]],\n",
       "\n",
       "\n",
       "       [[[0.72460233, 0.37623583, 0.54345175, 0.19508524]]],\n",
       "\n",
       "\n",
       "       [[[0.7581754 , 0.32659863, 0.5365549 , 0.17496355]]],\n",
       "\n",
       "\n",
       "       [[[0.69333409, 0.38518561, 0.57777841, 0.1925928 ]]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWBzSHElYSOK"
   },
   "source": [
    "# That's all Folks!\n",
    " \n",
    "Thank you so much for going through this tutorial. I am confident you will now be able to use AI to change the world and save our planet!\n",
    "\n",
    "If you have any more questions email me at : isitatalukdar@gmail.com\n",
    "\n",
    "---\n",
    "\n",
    "## Continue the Learning!\n",
    "\n",
    "Now that you have one classification technique down, let's try another, **Decision Trees**. We will also be discuss a way to analyze accuracy called a **Confusion Matrix**. \n",
    "\n",
    "\n",
    "## Decision Trees: Importing and Cleaning up the Data\n",
    "We already loaded the data in the beginning. Let's load it again, just in case you are starting to run the code from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dGLpk0AU-AA"
   },
   "outputs": [],
   "source": [
    "#Import required libraries \n",
    "import pandas as pd #loading data in table form  \n",
    "import numpy as np # linear algebra\n",
    "from sklearn.tree import DecisionTreeClassifier #Creating the Decision Tree\n",
    "from sklearn import tree#Visualizing the Decision Tree\n",
    "import graphviz #Visualizing the Decision Tree\n",
    "from sklearn.metrics import confusion_matrix #Confusion Matrix\n",
    "import matplotlib.pyplot as plt #visualization\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the dataset, which contains the examples and their labels\n",
    "iris_dataset = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1toi5XE1fkUC"
   },
   "source": [
    "## Decision Tree: Splitting up the training and test sets\n",
    "\n",
    "Remember that in classification, which is a type of supervised machine learning, we must use a training set to teach our model how to correctly classify future examples. We also use a test set to test how good our model is.\n",
    "\n",
    "The first step that we'll do is break up the Iris dataset into training set and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePM8-urmfu8a"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Break the dataset up into the examples (X) and their labels (y)\n",
    "X, y = iris_dataset.data, iris_dataset.target\n",
    "\n",
    "# Split up the X and y datasets into train and test sets\n",
    "# 25% of the dataset will be used for the test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=31)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxLj0Gp0g0ie"
   },
   "source": [
    "Our training set contains all of the correct targets (classes) for our flower examples, along with the four features of each flower. We'll need all of this information to teach our classifier how to predict a class given a new set of four features.\n",
    "\n",
    "**Note: Because the data points are split randomly into train and test, each run might not be the same**\n",
    "\n",
    "## Decision Tree: Training our Classifier\n",
    "\n",
    "As Decision Trees are a well-known classifier. There is already a library with a function to make one. You can always create it from scratch, but sometimes using the library function is less tedius. When you need to customize more advanced aspects of the classifier, it makes sense to start from scratch, unlike here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GFbQbOchhQZC"
   },
   "outputs": [],
   "source": [
    "# sklearn classifiers built in\n",
    "# We're going to import the decision tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize the classifier with a max_depth of 5\n",
    "classifier = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "classifier = classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mERzpovVhgw0"
   },
   "source": [
    "##Decision Tree: Visualizing the Classifier\n",
    "\n",
    "We can use some of the libraries to visualize our Descicion right here, instead of using a separate software. One of the major advantages with using a Decision Tree is that we can see visually how it makes its decisions. This **transparency** is not true for ANNs and is a very important research question currently.\n",
    "\n",
    "For example, machine learning is often used in government agencies today. If a model makes a decision that can affect whether someone gets health care, they should be able to justify *why* the system made the decision that it did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "8_9lQkluh7FY",
    "outputId": "9f33f8a6-208a-4868-8749-27bf41f49337"
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import graphviz\n",
    "\n",
    "dot_data = tree.export_graphviz(classifier, out_file=None, impurity=False) \n",
    "graph = graphviz.Source(dot_data) \n",
    "#Displays Graph\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOItK5BFiPaR"
   },
   "source": [
    "_Again, because we split the dataset randomly, this means that your decision tree may look different from another student's decision tree._\n",
    "\n",
    "Here's an example of what the top of the decision tree visualization may look like:\n",
    "\n",
    "![alt text](https://i.imgur.com/SFktGyk.png)\n",
    "\n",
    "It takes some practice to read the visualization of the decision tree, but it's actually quite informative. The root node of the decision tree starts with the line `X[3] <= 0.75`. This is the condition that splits the tree. In this case, it's saying that we should look at the feature with index 3 (petal width) and see if it's less than or equal to 0.75. If this is true, we will follow the tree to the left child. If this is false, we will follow the tree to the right child.\n",
    "\n",
    "The `samples = 112` line means that at this node, we still have 112 samples to look at.\n",
    "\n",
    "The `[39, 34, 39]` line tells us that of these 112 samples, 39 are the zeroth class (setosa), 34 are the first class (versicolor), and 39 are the second class (virginica).\n",
    "\n",
    "After the first split, we'll see that we did really well! All 39 samples of setosa are correctly classified in the left child of the root.\n",
    "\n",
    "We can follow the visualization for the rest of the decision tree to see what feature it splits on at each node.\n",
    "\n",
    "---\n",
    "##Decision Tree: Testing\n",
    "We've trained our decision tree and visualized it, but we have not yet tested it to see how well it does. This is where the test set comes in -- the test set is a set of correctly labelled examples that we have withheld from the decision tree, so we can test to see if the predictions made by the decision tree match the correct labels.\n",
    "\n",
    "With `sklearn`, it's really easy to generate our predicted labels for the test set:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6QVLVGRhio1l"
   },
   "outputs": [],
   "source": [
    "# Create a list of predicted classes for each of the examples in the test set\n",
    "y_predict = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAc18JTiiuyp"
   },
   "source": [
    "In order to find the accuracy of our classifier on the test set, we use the function `score()`, which takes two parameters: (1) the data of the test set, and (2) the correct labels of the test set.\n",
    "\n",
    "It will automatically compare our predicted label with the correct label to compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "GLkqywyTivxn",
    "outputId": "32bf5ca1-bde6-443d-83fa-b386e4db44b2"
   },
   "outputs": [],
   "source": [
    "accuracy = classifier.score(X_test, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eih8JYrSi8Mh"
   },
   "source": [
    "##Congrats!\n",
    "\n",
    "Now you've made a decision tree as well! But we are mssing something... Sure, we know the accuracy of the classifier, but what about the distributions of answers? False Positives? False Negatives? That kind of information is valuable for analyzing error and we can see it through a **confusion matrix**\n",
    "\n",
    "---\n",
    " ## Confusion Matrix\n",
    "\n",
    "Trust me, it's not as confusing as it sounds. Here we will display a confusion matrix to analyze our predictions.\n",
    "\n",
    "We'll use the familiar `matplotlib` library to accomplish this visualization, but we'll also use a library called `seaborn` to make our visualization look a bit nicer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "gAZ0tLgYj3ZH",
    "outputId": "0f2a536f-19cb-469f-ee8a-5038dc8be09e"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test,y_predict)\n",
    "cm_df = pd.DataFrame(\n",
    "    confusion_matrix, \n",
    "    index = [idx for idx in ['setosa', 'versicolor', 'virginica']],\n",
    "    columns = [col for col in ['setosa (pred)', 'versicolor (pred)', 'virginica (pred)']])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(cm_df, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVQ8h1aZkShE"
   },
   "source": [
    "Our confusion matrix in this case is a 3x3 table, because there are 3 different possible classes for each flower. The columns tell us what class we predicted, whereas the rows tell us what the actual class is.\n",
    "\n",
    "*Because we randomly split the data set, your confusion matrix might look different from another run.*\n",
    "\n",
    "The following is an example of what the confusion matrix might look like:\n",
    "\n",
    "![alt text](https://i.imgur.com/vsKCEKx.png)\n",
    "\n",
    "The first row tells us that for flowers that should actually be classified as setosa, what our decision tree predicted their class should be. In the example screenshot, there were 11 setosa flowers, and they were all correctly labelled setosa.\n",
    "\n",
    "The second row is more interesting. It tells us that there were 16 versicolor plants, but only 14 were classified correctly. The remaining two were predicted to be virginica, which was incorrect.\n",
    "\n",
    "Finally, the last row shows that our decision tree classified all 11 virginica plants correctly.\n",
    "\n",
    "For this particular example above, there were 38 test examples, and 36 were classified correctly, for an accuracy of 94.7%. The confusion matrix helps us visualize the performance of our decision tree and in addition to the accuracy number itself, it gives us the added information of which type of flower we tended to classify incorrectly.\n",
    "\n",
    "---\n",
    "#That's all Folks!\n",
    "\n",
    "Thank you so much for going through this tutorial. I am confident you will now be able to use AI to change the world and save our planet!\n",
    "\n",
    "If you have any more questions email me at : isitatalukdar@gmail.com\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
